{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x0000000003FF13C8>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of oauth_login() function is\n",
    "#    1. Login to the Twitter API\n",
    "#    2. Return twitter handle necessary for its use in later functions\n",
    "\n",
    "def oauth_login():\n",
    "    # Had to go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'zmbyPowgnW1QNs9LXjGtNqMcm'\n",
    "    CONSUMER_SECRET ='wnbgjQQJxidVvnw3CORSw5MVjbOE8KstdXlkPLrH01R9JLAlPn'\n",
    "    OAUTH_TOKEN = '1529017262-SoyYbWBJWW8Mn0MLuyQLpmViknAiGQg1zVJv3N7'\n",
    "    OAUTH_TOKEN_SECRET = '4nqZePogkTfNVBJqqb1VN0dpprz8ykeKBDrdxXsxDFYnC'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of save_to_mongo() function is\n",
    "#    1. Connect to the MongoDB server running on localhost\n",
    "#    2. Get a reference to database specified in input paramter\n",
    "#    3. Reference collection specified in input parameter\n",
    "#    4. Save data to database and return IDs\n",
    "\n",
    "def save_to_mongo(data, mongo_db, mongo_db_coll, **mongo_conn_kw):\n",
    "    \n",
    "    # Connects to the MongoDB server running on \n",
    "    # localhost:27017 by default\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    \n",
    "    # Get a reference to a particular database\n",
    "    \n",
    "    db = client[mongo_db]\n",
    "    \n",
    "    # Reference a particular collection in the database\n",
    "    \n",
    "    coll = db[mongo_db_coll]\n",
    "    \n",
    "    # Perform a bulk insert and  return the IDs\n",
    "    return coll.insert_many(data)\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of load_from_mongo() function is\n",
    "#    1. Given the db and coll passed in, find data based on criteria if given\n",
    "#    4. Load data from database and return cursor or item to user\n",
    "\n",
    "def load_from_mongo(mongo_db, mongo_db_coll, return_cursor=False,\n",
    "                    criteria=None, projection=None, **mongo_conn_kw):\n",
    "    \n",
    "    # Optionally, use criteria and projection to limit the data that is \n",
    "    # returned as documented in \n",
    "    # http://docs.mongodb.org/manual/reference/method/db.collection.find/\n",
    "    \n",
    "    # Consider leveraging MongoDB's aggregations framework for more \n",
    "    # sophisticated queries.\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    db = client[mongo_db]\n",
    "    coll = db[mongo_db_coll]\n",
    "    \n",
    "    if criteria is None:\n",
    "        criteria = {}\n",
    "    \n",
    "    if projection is None:\n",
    "        cursor = coll.find(criteria)\n",
    "    else:\n",
    "        cursor = coll.find(criteria, projection)\n",
    "\n",
    "    # Returning a cursor is recommended for large amounts of data\n",
    "    \n",
    "    if return_cursor:\n",
    "        return cursor\n",
    "    else:\n",
    "        return [ item for item in cursor ]\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the get_trends() function is:\n",
    "#    1. Gather current world trends from twitter\n",
    "#    2. Gather current US trends from twitter\n",
    "#    3. Print the trends for viewing (TODO: Do we want to save trending data?)\n",
    "\n",
    "def get_trends(twitter_api):\n",
    "    \n",
    "    # The Yahoo! Where On Earth ID for the entire world is 1.\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "    # http://developer.yahoo.com/geo/geoplanet/\n",
    "\n",
    "    WORLD_WOE_ID = 1\n",
    "    US_WOE_ID = 23424977\n",
    "\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special case keyword argument.\n",
    "\n",
    "    world_trends = twitter_api.trends.place(_id=WORLD_WOE_ID)\n",
    "    us_trends = twitter_api.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "\n",
    "    print ('*** Here are the WORLD TRENDS :')\n",
    "    print (json.dumps(world_trends, indent=1))\n",
    "    print ('*** Here are the US TRENDS :')\n",
    "    print (json.dumps(us_trends, indent=1))\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# OLD CODE - NO LONGER USED\n",
    "# The purpose of the getPokemonGoTweets() function is:\n",
    "#   1. Search twitter for tweets containing the search term '#PokemonGo'\n",
    "#   2. Save any tweets retrieved in a file named 'pokemonresults.txt'\n",
    "#\n",
    "# This was the original function written to get PokemonGo tweets.  It is no longer\n",
    "# being called by anyone, but was saved to show our first attempt using the polling API\n",
    "# and writing results to a file rather than a database.\n",
    "\n",
    "def getPokemonGoTweets(twitter_api):\n",
    "    \n",
    "       \n",
    "    # Import unquote to prevent url encoding errors in next_results\n",
    "    from urllib.parse import unquote\n",
    "\n",
    "    # Set q to the topic of interest for CaseStudy1-Problem1, PokemonGo \n",
    "\n",
    "    q = '#PokemonGo' \n",
    "\n",
    "    count = 100\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
    "\n",
    "    search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "\n",
    "    statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "    # Iterate through 5 more batches of results by following the cursor\n",
    "\n",
    "    for _ in range(5):\n",
    "        print (\"Length of statuses\", len(statuses))\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "    # Dump data to json file in json format    \n",
    "    with open('pokemonresults.txt', 'w') as outfile:\n",
    "        json.dump(statuses, outfile)  \n",
    "    # Note: by opening file using 'with', no need to close() the file. It happens implicitly.\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the twitter_search() function is:\n",
    "#   1. Search twitter for tweets containing the search term q, defaulting results=200\n",
    "#      because oAuth users can \"only\" make 180 search queries per 15 minute interval\n",
    "#\n",
    "#   2. return statuses\n",
    "\n",
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets and \n",
    "    # https://dev.twitter.com/docs/using-search for details on advanced \n",
    "    # search criteria that may be useful for keyword arguments\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://dev.twitter.com/docs/rate-limiting/1.1/limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "        \n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the mainDataGathering() function is:\n",
    "#     1. Login to twitter and get api handle\n",
    "#     1. Gather desired twitter data and save for future use\n",
    "\n",
    "def mainDataGathering():\n",
    "    \n",
    "    twitter_api = oauth_login()\n",
    "    print(twitter_api)\n",
    "    \n",
    "    # Search twitter for \"#PokemonGo\" tweets\n",
    "    q = \"#PokemonGo\"\n",
    "\n",
    "    # Use twitter streaming API\n",
    "    twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/streaming-apis\n",
    "    #stream = twitter_stream.statuses.filter(track=q)\n",
    "    \n",
    "    \n",
    "    #get_trends(twitter_api)\n",
    "\n",
    "    #getPokemonGoTweets(twitter_api)\n",
    "    \n",
    "    # MongoDB---------------------------------------------------\n",
    "    # Connects to the MongoDB server running on \n",
    "    # localhost:27017 by default\n",
    "    \n",
    "    #client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    client = pymongo.MongoClient('localhost', 27017)\n",
    "    \n",
    "    # Get a reference to a particular database\n",
    "    mongo_db = 'stream_search_results'\n",
    "    db = client[mongo_db]\n",
    "    \n",
    "    # Reference a particular collection in the database\n",
    "    mongo_db_coll = 'StreamingTweet'\n",
    "    coll = db[mongo_db_coll]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #results = twitter_search(twitter_api, q, max_results=10)\n",
    "    stream = twitter_stream.statuses.filter(track=q)\n",
    "    for tweet in stream:\n",
    "        #print(tweet['text'])  \n",
    "        # Save results to MongoDB\n",
    "        coll.insert_one(tweet)\n",
    "        #save_to_mongo(stream, 'stream_search_results', 'StreamingTweet')\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # This is an example of how to get some data from the database\n",
    "    #\n",
    "    #print('**** Load the DATABASE ***')\n",
    "    #cursor = load_from_mongo('stream_search_results', q, return_cursor=True)\n",
    "    #print('**** HERE is some data from the DATABASE ***')\n",
    "    #for trend in cursor:\n",
    "        # Print the text\n",
    "        # print(trend['text'])\n",
    "        # Print user and location\n",
    "        #print(trend['user']['location'])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Call the mainDataGathering() function to perform actions needed for \n",
    "# DS501 CaseStudy 1 Problem 1 : Sampling twitter data with streaming api about a topic\n",
    "\n",
    "mainDataGathering()\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Test that the mainDataGathering() worked by displaying a sample result from the file\n",
    "# This is not necessary and can be commented out if display is not desired here...\n",
    "\n",
    "#print (\"*** Sample PokemonGo output : \")\n",
    "# Get one sample search result from file\n",
    "#with open('pokemonresults.txt','r') as json_data:\n",
    "#    json_statuses = json.load(json_data)\n",
    "#    print(json.dumps(json_statuses[0], indent=4))\n",
    "# Note: by opening file using 'with', no need to close() the file. It happens implicitly.\n",
    "\n",
    "# q = \"#PokemonGo\"\n",
    "#load_from_mongo('stream_search_results', q)\n",
    "\n",
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "mongo_db = 'stream_search_results'\n",
    "mongo_db_coll = 'StreamingTweet'\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# This is an example of how to get some data from the database\n",
    "#\n",
    "print('**** Load the DATABASE ***')\n",
    "#cursor = load_from_mongo('stream_search_results', 'twitter', return_cursor=True)\n",
    "# Connects to the MongoDB server running on \n",
    "# localhost:27017 by default\n",
    "    \n",
    "client = pymongo.MongoClient()\n",
    "    \n",
    "# Get a reference to a particular database\n",
    "    \n",
    "db = client[mongo_db]\n",
    "    \n",
    "# Reference a particular collection in the database\n",
    "coll = db[mongo_db_coll]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "\n",
    "print('**** HERE is some data from the DATABASE ***')\n",
    "\n",
    "#for trend in cursor:\n",
    "    #print(trend['text'])\n",
    "    #print(trend['user']['location'])\n",
    "    \n",
    "\n",
    "     \n",
    "# This code is based on Example #10-13 in Chapter 9 of Mining the Social Web\n",
    "    \n",
    "\n",
    "\n",
    "curr_tweet_texts = [ curr_tweet['text'] \n",
    "                     for curr_tweet in cursor ]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] \n",
    "                 for curr_tweet in cursor\n",
    "                     for user_mention in curr_tweet['entities']['user_mentions'] ]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "hashtags = [ hashtag['text'] \n",
    "             for curr_tweet in cursor\n",
    "                 for hashtag in curr_tweet['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w \n",
    "          for t in curr_tweet_texts \n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print (json.dumps(curr_tweet_texts[0:5], indent=1))\n",
    "print (json.dumps(screen_names[0:5], indent=1)) \n",
    "print (json.dumps(hashtags[0:5], indent=1))\n",
    "print (json.dumps(words[0:5], indent=1))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print()\n",
    "print('*** And here are the TOP 30 : ')\n",
    "\n",
    "for item in [words]:\n",
    "    pt = PrettyTable(field_names=['Words', 'Count'])\n",
    "    c = Counter(item)\n",
    "    #print (c.most_common()[:30]) # top 30\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:30] ]\n",
    "    pt.align['Words'], pt.align['Count'] = 'l', 'r'\n",
    "    print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
